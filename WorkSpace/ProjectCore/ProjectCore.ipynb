{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRNG evolution analysis with prediction attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Basic imports for now\n",
    "\n",
    "# NN-Tools\n",
    "import numpy as np \n",
    "import keras \n",
    "\n",
    "# Visualization\n",
    "from IPython.display import SVG \n",
    "from IPython.display import display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Printing\n",
    "from sympy import *\n",
    "init_printing(use_latex=True)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 999, 1)\n",
      "[1289468076106719173646 1514931761581815331176 25239667299216193810\n",
      " 6139281543742180680128 7265592899876209625482 6008408012606781074439\n",
      " 7015359338503353141353 5179789406659106531391 4287580669255044914705\n",
      " 7711379492688840923686 9711099301099858424112 1193231595021142622074\n",
      " 2374815161642952243539 4088498294907097113600 6489947331756344988073\n",
      " 7557486626063204996264 5052784922681466116586 2293102405257116332201\n",
      " 4211972074405260763535 7307224858948951079642 9978500762451513867795\n",
      " 33929341591663264469 4797170560279851366678 17092486872711368773\n",
      " 42768617378640383544 7601753977013420669000 675472061175635152070\n",
      " 3140458889729114131022 5809505722119717480129 164693206825531722789\n",
      " 5032609283687849343527 86814363944179228889 6282458281719195548948\n",
      " 7174812461754380803413 3597793045084313958425 845230326119716581668\n",
      " 8874791401092704798453 4318523943681076668126 6204070497406569656290\n",
      " 8525003897320751390 5358298426176991148228 7343363363686775287929\n",
      " 3301815905698310950465 1235219565790527285102 1115108186739939095483\n",
      " 4195109391785397712056 6553313643361466829084 2542527589377233916435\n",
      " 6614585099354312196473 1338630050288045315614 1141257208651173897119\n",
      " 3159857474426682811979 74474888278985084424 61899507001728882155\n",
      " 8674797508049317976036 5492756948607162764057 8064427823735993760568\n",
      " 693017722707302311248 42669222953460923797 48128884753104766749\n",
      " 2604002011190934134040 1470489397372798373377 7588574037282105494017\n",
      " 8267646530020643760712 10759730935258805814 1170394948680963913979\n",
      " 6166242170295373898420 3151417649415890375604 7352436441583599622153\n",
      " 1641235230920386346074 7751269642478901152470 69332552425503415281\n",
      " 9764242201369073829953 2910838768561169626203 4335496582783553832856\n",
      " 6834609842422754539413 8806760773442739348902 3135048606610629765028\n",
      " 8634329467975295139352 2791161598908074915925 6629094359731861381706\n",
      " 1312644323879220627863 7940147082753869136262 8669306898893803867450\n",
      " 3049851277811715723791 5927911672535962210841 5710399882940002955493\n",
      " 5857593395312095617766 8168203845329125760128 5769438552366461302223\n",
      " 6428747074531192658 236786821994698355107 12385827803549835300\n",
      " 9462086084291369462490 2876549776557629473880 9788932173470009125745\n",
      " 7501676322931716772054 7570325896829440713755 8444060945215595395684\n",
      " 23030911492282472859 53856054689078277576 5608775915767601250369\n",
      " 3517196033862203338021 44121397153145071528 9032189246994278569609\n",
      " 65852306057598479007 5857638012564099561064 4568165198149852834853\n",
      " 7934699294053182172759 9899419314803089299394 3892982979514424229842\n",
      " 1510926066160620833 6485061440418447305870 458047545339885665447\n",
      " 9911119384695582531647 3311567338950272261178 4547655164707344633836\n",
      " 2493342697294154210637 2733463817703236696217 7010398711125589535\n",
      " 4781519583981159618255 7503986589545967649245 3022392734466981723545\n",
      " 4788113579811381660310 5584277753502007602504 8862182831221657881059\n",
      " 1767086635568314496591 1904691846195889435556 4918448163344945965889\n",
      " 8249111018853233884399 3462404913142030624899 1246281760871354321907\n",
      " 9279690271775492010509 5063266492346098798477 6482795663755456172015\n",
      " 9836581274952140252680 4342111112525322111411 63726766840550386471\n",
      " 3799642111366540107952 6686777507650794517138 271253945163794346010\n",
      " 8023352159693460707567 8431521856494218988 9649842995754389788861\n",
      " 52654752972922180603 8589686372916069333105 8288433273611994741\n",
      " 6920824824598011076402 1320439130551894778275 7426720856006417919220\n",
      " 6779467084925753832832 2798071559376551511920 4159931227746920626203\n",
      " 43586883666751145096 275609286639633076560 9462853853859838676317\n",
      " 1091173395552867207404 805722989043503373502 5849825839964887449576\n",
      " 3307713746543067188385 3192883567260702517898 7896445654233843920132\n",
      " 3179080703490356501211 7874158254172655604841 5941481560484904979617\n",
      " 5419313245261339687994 1955883785536632381976 2046201050289872196933\n",
      " 868978833192505292653 3592237593896838670274 4797671993359832783013\n",
      " 2253826359838689761034 8431640666280177953170 7387929453808306614193\n",
      " 9852111259873233084645 5157586384864360089321 3793002433434604529702\n",
      " 3723669914831275565593 1588707096963955492926 995005182749920026895\n",
      " 2153094416291071241643 8719284836420969461931 3366666494622740753626\n",
      " 833355641146873651342 73710502012566855487 2989509084621503056433\n",
      " 5480580400439703592552 48074186638136578007 1354985487034735500483\n",
      " 7212861339473111049832 2362607140571002018932 11529253884391310940\n",
      " 5059963351655735207561 4670145520444036602143 1701650883351566637960\n",
      " 5080446710455056296300 6294226004838356748007 1104337206606945740491\n",
      " 1668070383136636020954 1273217885220145189999 5935226617390495163352\n",
      " 5780030384261361797020 771864178571723105966 98776146983103628379\n",
      " 8370000524785210291696 2609384035399658354533 6608810917559953169313\n",
      " 99829817547183431260 6657278434466020970972 6251409005395452675744\n",
      " 3986899333179357669364 9337016772813325648807 7252404949354283000450\n",
      " 9147939262585952195412 2908589414657386727758 6812163278627472602433\n",
      " 514110938207080204660 15098430513981138710 8910691063849979022933\n",
      " 8575986060014058215477 5463324608330261996724 9794449438768424760755\n",
      " 634554495703298741913 908673078356823456751 4336663492191390037361\n",
      " 7201716145698132765820 9679945938312545943597 8594721421505437321219\n",
      " 1788723347604941659517 907354258473066864100 3314986867050653833214\n",
      " 9890778950458275647053 3892826676876016494493 4423779692010228117002\n",
      " 826501188024089503 5479966641823292933837 4978672570025815924741\n",
      " 4248941495946961288162 9917100737131698223134 4036649812338225172466\n",
      " 34256012664291677572 1722812537785001400539 4262564275075332805759\n",
      " 188358738400164589403 2029366345654272935937 3602223238437128260755\n",
      " 5247319679796165990118 9711069155696008709670 3538690270632041206039\n",
      " 155668055266240018118 6531662973008815975908 4812809360709856873090\n",
      " 35069249493097340296 314736095080967676376 31292821174674575399\n",
      " 6703869409875684900823 3508359119765189092543 2864267490480119124665\n",
      " 5746648626823126677245 6710424234105936422439 1840274845446181894162\n",
      " 8115471376201835464035 5955573814626172207388 39041435416340846959\n",
      " 9402919941779732660534 7898326060665469574017 7312010241547697016117\n",
      " 5455643450597205427963 2157640941112810327846 7649917560211335915565\n",
      " 5743001775357529609813 93484294179256716926 3777765618775388520518\n",
      " 4656835090546150916277 6676157066108540603437 8576662609715864475245\n",
      " 4875142585503167939371 88464350870927470611 18060989704185742364\n",
      " 6373715485714564871130 7016704183857222351144 3460650223998463949050\n",
      " 5948104569706762447044 7486468189558170888085 5315055401928860117860\n",
      " 9078794423684300618187 5086741823054934775260 30933194412623071082\n",
      " 2349621795593830995777 430753694047910611833 9708558704895276208328\n",
      " 9083331733031697511114 7195896126816855741863 8647903246254200770309\n",
      " 5864736588525896778653 1495233259913865658597 4220543253625838565893\n",
      " 4408711905714256787908 9636386783305576840255 8861375970586079683263\n",
      " 257349429381713977603 9503977003990702748005 6830653577841906808870\n",
      " 56531172307918679336 2765378062937019016690 5912570474283075586800\n",
      " 7448049993186421999360 8075171844570398975745 452178661663956114224\n",
      " 60368156607373205022 3564180358960334956692 3244354278140476910260\n",
      " 4650329501238279026454 6746459201046551903731 1984857577772437451126\n",
      " 6642541021045286189777 5479933084636491655804 3799149083384277287203\n",
      " 8229805565303690638912 6606534445917347300631 1822144631832799638297\n",
      " 2944863545696542148965 60907246199251517234 5879662643384312622986\n",
      " 6187503105769825790415 8494350642120553139546 5991539320304376518326\n",
      " 671393677850921834873 6458584812362872367360 321804587192170575177\n",
      " 5789590587532071444909 4699094145083274541127 1642424382278872374622\n",
      " 1562811869866675690785 2410885199990168186748 6774099505138505693512\n",
      " 1178043055647787583321 4474065147019538555900 1865297984870995495444\n",
      " 6094918915185773660866 3425550356055239988941 3084030167007505134888\n",
      " 514792948729972463549 6684391932680017191999 79502781618931616788\n",
      " 3377325389850083380099 7742960300857264362448 8288121497511986593034\n",
      " 6402187706449245517243 463569756766677247583 5119365397690196176908\n",
      " 414632458609152324016 8256439377697661162786 711360864445686668115\n",
      " 3615641285130959079447 3005377442624573747959 4386060232621526440969\n",
      " 6757045961015761092477 5876474964011730064164 8921146410662790864277\n",
      " 8814653585809763492329 5667693486072547017389 6233948320336475323163\n",
      " 9084382245084188154576 2717918618835967036848 1416203812092693841267\n",
      " 493387518951396361781 52408630219887384926 3400415713916503467282\n",
      " 9306744808399932628783 1702142390140953968986 8881531824039771016206\n",
      " 1958028091766547086933 4787396242725317410324 8419248349293461910724\n",
      " 3024047105003301058216 58717287709226198603 15821074373256554175\n",
      " 1774514207217515364321 5045697868105982451580 8662206859253468847732\n",
      " 5556848791009006779595 2628225983661987654137 3637053683900054912644\n",
      " 92388158866573422251 6781036918238974080273 5098692688122786510564\n",
      " 1123671041226898314459 4194118660732433660954 8137762750015216249736\n",
      " 4185688351515393341515 1772751854419148719484 2561576369279062251741\n",
      " 7354036100097705082968 9695550718571462223374 684543250637004986\n",
      " 6634301341052255008415 15287007281158248581 3017914596542026876419\n",
      " 5334554509861202363163 91273636260799721530 2087068657612475720271\n",
      " 1848967697499983194644 2185172318850871833157 9455287603571752861741\n",
      " 7865943205115981416303 7224105091644871872928 7249231815776921712836\n",
      " 1567349836888686822872 6740664613734459026078 881715489434343122220\n",
      " 1148713922893170940278 2634021355106791250199 83107105400122015418\n",
      " 1510179336357113769687 4068494081366718440 1705835138310428884783\n",
      " 5550312628302560015062 5280407980500135600126 2965968507173116320757\n",
      " 3138733319360158066487 725305492603150485703 7949627885936642157408\n",
      " 6048363255140381204980 4130342297281617521165 3588984547973962340242\n",
      " 2402455202770635187285 4104187481204502945418 7243127526538445033003\n",
      " 3594672772270719042178 709424945586267944022 3719830998020420612760\n",
      " 8874525937935377654720 69984489054089460078 6811942715712658221555\n",
      " 4188995028217814741554 4198652108869224610818 5381849828405893451040\n",
      " 3954475684096773156591 1042669214013744430082 3388821527797675117771\n",
      " 5007754169736273715226 3278144952436886069442 5829379988569895745054\n",
      " 8735280666584402732793 3400630015225316700920 9947733907743616073402\n",
      " 5681859886974978822248 7961502837670203157508 7975189581582623454001\n",
      " 4821989102636900352322 5015864150032140931249 2339907180283920553307\n",
      " 4327198295862053286539 7277021136870063236509 2301614651067401066005\n",
      " 778140174393142360415 7097777270095224542938 3868691200259358645415\n",
      " 1197504438089631109420 1596580988881585379896 7136115976868119316592\n",
      " 3724215280019634643296 8119642692562854048296 669804242170018182434\n",
      " 5180155111560518178887 7248183702507750846340 6367642774564056126900\n",
      " 564987941058102591209 8678870758245160054417 8913685678466504925839\n",
      " 5734351347072900085116 9021764948757660687837 704778544696225196837\n",
      " 34395829612509828043 1350952148257966732407 7406822953185582957\n",
      " 2091061470171884072866 97495402897080555330 3159030029989966602635\n",
      " 5276910433174799385165 2697842719431779636017 4090775230211743242364\n",
      " 3087236753515775217772 7050875055236595331160 6833854468504191188549\n",
      " 43844441498036760894 88815906678755118904 5839443585142697991174\n",
      " 9398610892445038140581 4986564902074584607627 701591021718371780569\n",
      " 50651938274910503450 1327773235204360881472 1480924492512379675907\n",
      " 7927906388103781668065 3576127624683913250816 3046068536136292024283\n",
      " 5216329039093654018101 30953525016421262755]\n"
     ]
    }
   ],
   "source": [
    "from PRNGs import *\n",
    "from SeedGenerator import *\n",
    "import time\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "def split_data(x,y, numsets, setlength):\n",
    "    temp = []\n",
    "    for i in range(numsets):\n",
    "        t = ticks()\n",
    "        temp = Middle_Square(t,setlength)\n",
    "        for a in range(len(temp)):\n",
    "            temp[a] = int(temp[a])\n",
    "        n = temp[-1]\n",
    "        y.append(n)\n",
    "        x.append(temp[:-1])\n",
    "             \n",
    "\n",
    "split_data(x_train, y_train, 500, 1000)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "split_data(x_test, y_test, 500, 1000)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train)\n",
    "#print(y_train.shape)\n",
    "\n",
    "#print(x_test.shape)\n",
    "#print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv1D)              (None, 998, 500)          1500      \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv1D)              (None, 989, 100)          500100    \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv1D)              (None, 970, 50)           100050    \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv1D)              (None, 921, 20)           50020     \n",
      "_________________________________________________________________\n",
      "maxpool_1 (MaxPooling1D)     (None, 921, 20)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18420)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 73684     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 725,359\n",
      "Trainable params: 725,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def adversary(input_width, output_width):\n",
    "    \"\"\" Returns a keras sequential model.\n",
    "    :param input_width: the size of the input layer\n",
    "    :param output_width: the size of the output layer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv1D(500, kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                input_shape=(input_width, 1),\n",
    "                                name='conv_1'\n",
    "                             )\n",
    "         )\n",
    "    model.add(keras.layers.Conv1D(100, 10, activation='relu', name='conv_2'))\n",
    "    model.add(keras.layers.Conv1D(50, 20, activation='relu', name='conv_3'))\n",
    "    model.add(keras.layers.Conv1D(20, 50, activation='relu', name='conv_4'))\n",
    "    model.add(keras.layers.MaxPool1D(1, 1, name='maxpool_1'))\n",
    "    model.add(keras.layers.Flatten(name='flatten_1'))\n",
    "    model.add(keras.layers.Dense(4, activation='relu', name='dense_1'))\n",
    "    model.add(keras.layers.Dense(output_width, activation='relu', name='dense_2'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "mod = adversary(999,1)\n",
    "          \n",
    "mod.compile(loss='sparse_categorical_crossentropy', optimizer='adam', #https://jovianlin.io/cat-crossentropy-vs-sparse-cat-crossentropy/\n",
    "    metrics=['accuracy'])\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of -9223372036854775808 which is outside the valid range of [0, 1).  Label values: -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808\n\t [[node loss_4/dense_2_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_12556]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5d81ab239d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Received a label value of -9223372036854775808 which is outside the valid range of [0, 1).  Label values: -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808\n\t [[node loss_4/dense_2_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_12556]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "batch_size=5\n",
    "epochs=5\n",
    "history=mod.fit(x_train,y_train,batch_size=batch_size,epochs=epochs, validation_split=.3,verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "# summarize history for loss  \n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148],\n",
       "       [0.7328148]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Reshape\n",
    "'''\n",
    "#initially using fewer kernels and increasing gradually while monitoring the error rate on how it is varying\n",
    "model = keras.Sequential()\n",
    "model.add(Reshape((x.shape[1], 1), input_shape=(x.shape[1], )))\n",
    "model.add(keras.layers.Conv1D(98, kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                input_shape=(1,98)\n",
    "                             )\n",
    "         )\n",
    "        \n",
    "        \n",
    "#data work in progress\n",
    "          \n",
    "model.add(keras.layers.Conv1D(64, 2, activation='relu'))\n",
    "model.add(keras.layers.Conv1D(64, 2, activation='relu'))\n",
    "model.add(keras.layers.Conv1D(64, 2, activation='relu'))\n",
    "\n",
    "#test pool size and monitor what works better\n",
    "\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=(4)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "          \n",
    "          \n",
    "model.add(keras.layers.Dense(1, activation='softmax'))\n",
    "          \n",
    "    #data work in progress\n",
    "          \n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "          \n",
    "model.summary()\n",
    "          \n",
    "'''    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
