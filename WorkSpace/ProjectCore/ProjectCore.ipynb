{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRNG evolution analysis with prediction attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Basic imports for now\n",
    "\n",
    "# NN-Tools\n",
    "import numpy as np \n",
    "import keras \n",
    "\n",
    "# Visualization\n",
    "from IPython.display import SVG \n",
    "from IPython.display import display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Printing\n",
    "from sympy import *\n",
    "init_printing(use_latex=True)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 999, 1)\n",
      "[1289468076106719173646 1514931761581815331176 25239667299216193810\n",
      " 6139281543742180680128 7265592899876209625482 6008408012606781074439\n",
      " 7015359338503353141353 5179789406659106531391 4287580669255044914705\n",
      " 7711379492688840923686 9711099301099858424112 1193231595021142622074\n",
      " 2374815161642952243539 4088498294907097113600 6489947331756344988073\n",
      " 7557486626063204996264 5052784922681466116586 2293102405257116332201\n",
      " 4211972074405260763535 7307224858948951079642 9978500762451513867795\n",
      " 33929341591663264469 4797170560279851366678 17092486872711368773\n",
      " 42768617378640383544 7601753977013420669000 675472061175635152070\n",
      " 3140458889729114131022 5809505722119717480129 164693206825531722789\n",
      " 5032609283687849343527 86814363944179228889 6282458281719195548948\n",
      " 7174812461754380803413 3597793045084313958425 845230326119716581668\n",
      " 8874791401092704798453 4318523943681076668126 6204070497406569656290\n",
      " 8525003897320751390 5358298426176991148228 7343363363686775287929\n",
      " 3301815905698310950465 1235219565790527285102 1115108186739939095483\n",
      " 4195109391785397712056 6553313643361466829084 2542527589377233916435\n",
      " 6614585099354312196473 1338630050288045315614 1141257208651173897119\n",
      " 3159857474426682811979 74474888278985084424 61899507001728882155\n",
      " 8674797508049317976036 5492756948607162764057 8064427823735993760568\n",
      " 693017722707302311248 42669222953460923797 48128884753104766749\n",
      " 2604002011190934134040 1470489397372798373377 7588574037282105494017\n",
      " 8267646530020643760712 10759730935258805814 1170394948680963913979\n",
      " 6166242170295373898420 3151417649415890375604 7352436441583599622153\n",
      " 1641235230920386346074 7751269642478901152470 69332552425503415281\n",
      " 9764242201369073829953 2910838768561169626203 4335496582783553832856\n",
      " 6834609842422754539413 8806760773442739348902 3135048606610629765028\n",
      " 8634329467975295139352 2791161598908074915925 6629094359731861381706\n",
      " 1312644323879220627863 7940147082753869136262 8669306898893803867450\n",
      " 3049851277811715723791 5927911672535962210841 5710399882940002955493\n",
      " 5857593395312095617766 8168203845329125760128 5769438552366461302223\n",
      " 6428747074531192658 236786821994698355107 12385827803549835300\n",
      " 9462086084291369462490 2876549776557629473880 9788932173470009125745\n",
      " 7501676322931716772054 7570325896829440713755 8444060945215595395684\n",
      " 23030911492282472859 53856054689078277576 5608775915767601250369\n",
      " 3517196033862203338021 44121397153145071528 9032189246994278569609\n",
      " 65852306057598479007 5857638012564099561064 4568165198149852834853\n",
      " 7934699294053182172759 9899419314803089299394 3892982979514424229842\n",
      " 1510926066160620833 6485061440418447305870 458047545339885665447\n",
      " 9911119384695582531647 3311567338950272261178 4547655164707344633836\n",
      " 2493342697294154210637 2733463817703236696217 7010398711125589535\n",
      " 4781519583981159618255 7503986589545967649245 3022392734466981723545\n",
      " 4788113579811381660310 5584277753502007602504 8862182831221657881059\n",
      " 1767086635568314496591 1904691846195889435556 4918448163344945965889\n",
      " 8249111018853233884399 3462404913142030624899 1246281760871354321907\n",
      " 9279690271775492010509 5063266492346098798477 6482795663755456172015\n",
      " 9836581274952140252680 4342111112525322111411 63726766840550386471\n",
      " 3799642111366540107952 6686777507650794517138 271253945163794346010\n",
      " 8023352159693460707567 8431521856494218988 9649842995754389788861\n",
      " 52654752972922180603 8589686372916069333105 8288433273611994741\n",
      " 6920824824598011076402 1320439130551894778275 7426720856006417919220\n",
      " 6779467084925753832832 2798071559376551511920 4159931227746920626203\n",
      " 43586883666751145096 275609286639633076560 9462853853859838676317\n",
      " 1091173395552867207404 805722989043503373502 5849825839964887449576\n",
      " 3307713746543067188385 3192883567260702517898 7896445654233843920132\n",
      " 3179080703490356501211 7874158254172655604841 5941481560484904979617\n",
      " 5419313245261339687994 1955883785536632381976 2046201050289872196933\n",
      " 868978833192505292653 3592237593896838670274 4797671993359832783013\n",
      " 2253826359838689761034 8431640666280177953170 7387929453808306614193\n",
      " 9852111259873233084645 5157586384864360089321 3793002433434604529702\n",
      " 3723669914831275565593 1588707096963955492926 995005182749920026895\n",
      " 2153094416291071241643 8719284836420969461931 3366666494622740753626\n",
      " 833355641146873651342 73710502012566855487 2989509084621503056433\n",
      " 5480580400439703592552 48074186638136578007 1354985487034735500483\n",
      " 7212861339473111049832 2362607140571002018932 11529253884391310940\n",
      " 5059963351655735207561 4670145520444036602143 1701650883351566637960\n",
      " 5080446710455056296300 6294226004838356748007 1104337206606945740491\n",
      " 1668070383136636020954 1273217885220145189999 5935226617390495163352\n",
      " 5780030384261361797020 771864178571723105966 98776146983103628379\n",
      " 8370000524785210291696 2609384035399658354533 6608810917559953169313\n",
      " 99829817547183431260 6657278434466020970972 6251409005395452675744\n",
      " 3986899333179357669364 9337016772813325648807 7252404949354283000450\n",
      " 9147939262585952195412 2908589414657386727758 6812163278627472602433\n",
      " 514110938207080204660 15098430513981138710 8910691063849979022933\n",
      " 8575986060014058215477 5463324608330261996724 9794449438768424760755\n",
      " 634554495703298741913 908673078356823456751 4336663492191390037361\n",
      " 7201716145698132765820 9679945938312545943597 8594721421505437321219\n",
      " 1788723347604941659517 907354258473066864100 3314986867050653833214\n",
      " 9890778950458275647053 3892826676876016494493 4423779692010228117002\n",
      " 826501188024089503 5479966641823292933837 4978672570025815924741\n",
      " 4248941495946961288162 9917100737131698223134 4036649812338225172466\n",
      " 34256012664291677572 1722812537785001400539 4262564275075332805759\n",
      " 188358738400164589403 2029366345654272935937 3602223238437128260755\n",
      " 5247319679796165990118 9711069155696008709670 3538690270632041206039\n",
      " 155668055266240018118 6531662973008815975908 4812809360709856873090\n",
      " 35069249493097340296 314736095080967676376 31292821174674575399\n",
      " 6703869409875684900823 3508359119765189092543 2864267490480119124665\n",
      " 5746648626823126677245 6710424234105936422439 1840274845446181894162\n",
      " 8115471376201835464035 5955573814626172207388 39041435416340846959\n",
      " 9402919941779732660534 7898326060665469574017 7312010241547697016117\n",
      " 5455643450597205427963 2157640941112810327846 7649917560211335915565\n",
      " 5743001775357529609813 93484294179256716926 3777765618775388520518\n",
      " 4656835090546150916277 6676157066108540603437 8576662609715864475245\n",
      " 4875142585503167939371 88464350870927470611 18060989704185742364\n",
      " 6373715485714564871130 7016704183857222351144 3460650223998463949050\n",
      " 5948104569706762447044 7486468189558170888085 5315055401928860117860\n",
      " 9078794423684300618187 5086741823054934775260 30933194412623071082\n",
      " 2349621795593830995777 430753694047910611833 9708558704895276208328\n",
      " 9083331733031697511114 7195896126816855741863 8647903246254200770309\n",
      " 5864736588525896778653 1495233259913865658597 4220543253625838565893\n",
      " 4408711905714256787908 9636386783305576840255 8861375970586079683263\n",
      " 257349429381713977603 9503977003990702748005 6830653577841906808870\n",
      " 56531172307918679336 2765378062937019016690 5912570474283075586800\n",
      " 7448049993186421999360 8075171844570398975745 452178661663956114224\n",
      " 60368156607373205022 3564180358960334956692 3244354278140476910260\n",
      " 4650329501238279026454 6746459201046551903731 1984857577772437451126\n",
      " 6642541021045286189777 5479933084636491655804 3799149083384277287203\n",
      " 8229805565303690638912 6606534445917347300631 1822144631832799638297\n",
      " 2944863545696542148965 60907246199251517234 5879662643384312622986\n",
      " 6187503105769825790415 8494350642120553139546 5991539320304376518326\n",
      " 671393677850921834873 6458584812362872367360 321804587192170575177\n",
      " 5789590587532071444909 4699094145083274541127 1642424382278872374622\n",
      " 1562811869866675690785 2410885199990168186748 6774099505138505693512\n",
      " 1178043055647787583321 4474065147019538555900 1865297984870995495444\n",
      " 6094918915185773660866 3425550356055239988941 3084030167007505134888\n",
      " 514792948729972463549 6684391932680017191999 79502781618931616788\n",
      " 3377325389850083380099 7742960300857264362448 8288121497511986593034\n",
      " 6402187706449245517243 463569756766677247583 5119365397690196176908\n",
      " 414632458609152324016 8256439377697661162786 711360864445686668115\n",
      " 3615641285130959079447 3005377442624573747959 4386060232621526440969\n",
      " 6757045961015761092477 5876474964011730064164 8921146410662790864277\n",
      " 8814653585809763492329 5667693486072547017389 6233948320336475323163\n",
      " 9084382245084188154576 2717918618835967036848 1416203812092693841267\n",
      " 493387518951396361781 52408630219887384926 3400415713916503467282\n",
      " 9306744808399932628783 1702142390140953968986 8881531824039771016206\n",
      " 1958028091766547086933 4787396242725317410324 8419248349293461910724\n",
      " 3024047105003301058216 58717287709226198603 15821074373256554175\n",
      " 1774514207217515364321 5045697868105982451580 8662206859253468847732\n",
      " 5556848791009006779595 2628225983661987654137 3637053683900054912644\n",
      " 92388158866573422251 6781036918238974080273 5098692688122786510564\n",
      " 1123671041226898314459 4194118660732433660954 8137762750015216249736\n",
      " 4185688351515393341515 1772751854419148719484 2561576369279062251741\n",
      " 7354036100097705082968 9695550718571462223374 684543250637004986\n",
      " 6634301341052255008415 15287007281158248581 3017914596542026876419\n",
      " 5334554509861202363163 91273636260799721530 2087068657612475720271\n",
      " 1848967697499983194644 2185172318850871833157 9455287603571752861741\n",
      " 7865943205115981416303 7224105091644871872928 7249231815776921712836\n",
      " 1567349836888686822872 6740664613734459026078 881715489434343122220\n",
      " 1148713922893170940278 2634021355106791250199 83107105400122015418\n",
      " 1510179336357113769687 4068494081366718440 1705835138310428884783\n",
      " 5550312628302560015062 5280407980500135600126 2965968507173116320757\n",
      " 3138733319360158066487 725305492603150485703 7949627885936642157408\n",
      " 6048363255140381204980 4130342297281617521165 3588984547973962340242\n",
      " 2402455202770635187285 4104187481204502945418 7243127526538445033003\n",
      " 3594672772270719042178 709424945586267944022 3719830998020420612760\n",
      " 8874525937935377654720 69984489054089460078 6811942715712658221555\n",
      " 4188995028217814741554 4198652108869224610818 5381849828405893451040\n",
      " 3954475684096773156591 1042669214013744430082 3388821527797675117771\n",
      " 5007754169736273715226 3278144952436886069442 5829379988569895745054\n",
      " 8735280666584402732793 3400630015225316700920 9947733907743616073402\n",
      " 5681859886974978822248 7961502837670203157508 7975189581582623454001\n",
      " 4821989102636900352322 5015864150032140931249 2339907180283920553307\n",
      " 4327198295862053286539 7277021136870063236509 2301614651067401066005\n",
      " 778140174393142360415 7097777270095224542938 3868691200259358645415\n",
      " 1197504438089631109420 1596580988881585379896 7136115976868119316592\n",
      " 3724215280019634643296 8119642692562854048296 669804242170018182434\n",
      " 5180155111560518178887 7248183702507750846340 6367642774564056126900\n",
      " 564987941058102591209 8678870758245160054417 8913685678466504925839\n",
      " 5734351347072900085116 9021764948757660687837 704778544696225196837\n",
      " 34395829612509828043 1350952148257966732407 7406822953185582957\n",
      " 2091061470171884072866 97495402897080555330 3159030029989966602635\n",
      " 5276910433174799385165 2697842719431779636017 4090775230211743242364\n",
      " 3087236753515775217772 7050875055236595331160 6833854468504191188549\n",
      " 43844441498036760894 88815906678755118904 5839443585142697991174\n",
      " 9398610892445038140581 4986564902074584607627 701591021718371780569\n",
      " 50651938274910503450 1327773235204360881472 1480924492512379675907\n",
      " 7927906388103781668065 3576127624683913250816 3046068536136292024283\n",
      " 5216329039093654018101 30953525016421262755]\n"
     ]
    }
   ],
   "source": [
    "from PRNGs import *\n",
    "from SeedGenerator import *\n",
    "import time\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "def split_data(x,y, numsets, setlength):\n",
    "    temp = []\n",
    "    for i in range(numsets):\n",
    "        t = ticks()\n",
    "        temp = Middle_Square(t,setlength)\n",
    "        for a in range(len(temp)):\n",
    "            temp[a] = int(temp[a])\n",
    "        n = temp[-1]\n",
    "        y.append(n)\n",
    "        x.append(temp[:-1])\n",
    "             \n",
    "\n",
    "split_data(x_train, y_train, 500, 1000)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "split_data(x_test, y_test, 500, 1000)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train)\n",
    "#print(y_train.shape)\n",
    "\n",
    "#print(x_test.shape)\n",
    "#print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv1D)              (None, 998, 500)          1500      \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv1D)              (None, 989, 100)          500100    \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv1D)              (None, 970, 50)           100050    \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv1D)              (None, 921, 20)           50020     \n",
      "_________________________________________________________________\n",
      "maxpool_1 (MaxPooling1D)     (None, 921, 20)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18420)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 73684     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 725,359\n",
      "Trainable params: 725,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def adversary(input_width, output_width):\n",
    "    \"\"\" Returns a keras sequential model.\n",
    "    :param input_width: the size of the input layer\n",
    "    :param output_width: the size of the output layer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv1D(500, kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                input_shape=(input_width, 1),\n",
    "                                name='conv_1'\n",
    "                             )\n",
    "         )\n",
    "    model.add(keras.layers.Conv1D(100, 10, activation='relu', name='conv_2'))\n",
    "    model.add(keras.layers.Conv1D(50, 20, activation='relu', name='conv_3'))\n",
    "    model.add(keras.layers.Conv1D(20, 50, activation='relu', name='conv_4'))\n",
    "    model.add(keras.layers.MaxPool1D(1, 1, name='maxpool_1'))\n",
    "    model.add(keras.layers.Flatten(name='flatten_1'))\n",
    "    model.add(keras.layers.Dense(4, activation='relu', name='dense_1'))\n",
    "    model.add(keras.layers.Dense(output_width, activation='relu', name='dense_2'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "mod = adversary(999,1)\n",
    "          \n",
    "mod.compile(loss='sparse_categorical_crossentropy', optimizer='adam', #https://jovianlin.io/cat-crossentropy-vs-sparse-cat-crossentropy/\n",
    "    metrics=['accuracy'])\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 500 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of -9223372036854775808 which is outside the valid range of [0, 1).  Label values: -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808\n\t [[node loss_4/dense_2_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_12556]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5d81ab239d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Received a label value of -9223372036854775808 which is outside the valid range of [0, 1).  Label values: -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808 -9223372036854775808\n\t [[node loss_4/dense_2_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_12556]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "batch_size=5\n",
    "epochs=5\n",
    "history=mod.fit(x_train,y_train,batch_size=batch_size,epochs=epochs, validation_split=.3,verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "# summarize history for loss  \n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deprecated for now .........\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Reshape\n",
    "'''\n",
    "#initially using fewer kernels and increasing gradually while monitoring the error rate on how it is varying\n",
    "model = keras.Sequential()\n",
    "model.add(Reshape((x.shape[1], 1), input_shape=(x.shape[1], )))\n",
    "model.add(keras.layers.Conv1D(98, kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                input_shape=(1,98)\n",
    "                             )\n",
    "         )\n",
    "        \n",
    "        \n",
    "#data work in progress\n",
    "          \n",
    "model.add(keras.layers.Conv1D(64, 2, activation='relu'))\n",
    "model.add(keras.layers.Conv1D(64, 2, activation='relu'))\n",
    "model.add(keras.layers.Conv1D(64, 2, activation='relu'))\n",
    "\n",
    "#test pool size and monitor what works better\n",
    "\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=(4)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "          \n",
    "          \n",
    "model.add(keras.layers.Dense(1, activation='softmax'))\n",
    "          \n",
    "    #data work in progress\n",
    "          \n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "          \n",
    "model.summary()\n",
    "          \n",
    "'''    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
